\subsection{Evaluation Guidelines}
Kleese et al. \cite{Klees.2018} conclude several good/necessary practices for experimental evaluation of fuzzers.
%
We summarize the main points and try to follow their advice in our evaluation.
%
First, the relative performance between two fuzzers can change over time, so shorter experiments have higher risk of showing misleading results.
%
Second, multiple trials should be done for statistical validity.
%
Finally, due diligence must be made in reporting the details of evaluation, which helps with reproducibility of the results.
%
For example, one should clearly report the performance measures used, the baselines, fuzz targets, fuzzer parameters such as seeds and timeout, the number of trials, and the statistics used to analyze/summarize the results.


Herrera et al. \cite{Herrera.2021} investigate the role of seeds in fuzzing performance and give advice on comparing fuzzers.
%
They conclude that the choice of seeds can have a significant impact on a fuzzer's performance, so it should be clearly documented in an experimental evaluation.
%
Also, one should never use only a single seed.


The recent reproducibility crisis \cite{Baker.2016}, \cite{Oza.2023} in the sciences have highlighted some questionable scientific experiment practices.
%
We are aware of some of these problems which are rooted in frequentist statistics \cite{Jaynes.2003}, \cite{Clayton.2021}.
%
As such, we intentionally did not perform any statistical tests since we are not sure which bayesian method would be informative to summarize our results.